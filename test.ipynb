{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a1224e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d27c9b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prefix = 'input'\n",
    "# want test code to take SparkSession as input because we want this to be able to run on a spark cluster\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from etl.load import load\n",
    "from etl.clean import nullify_missing_values, parse_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53e73b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "countrylist, stationlist, data = load(spark=spark, input_prefix=input_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-----+--------+----+----+------+------+-----+----+-----+-----+----+----+----+-----+------+\n|STN---| WBAN|YEARMODA|TEMP|DEWP|   SLP|   STP|VISIB|WDSP|MXSPD| GUST| MAX| MIN|PRCP| SNDP|FRSHTT|\n+------+-----+--------+----+----+------+------+-----+----+-----+-----+----+----+----+-----+------+\n| 10260|99999|20190101|26.1|21.2|1001.9| 987.5| 20.6| 9.0| 15.9| 29.7|29.8|null|null| 18.5|001000|\n| 10260|99999|20190102|24.9|22.1|1020.1|1005.5|  5.4| 5.6| 13.6| 22.1|null|20.7|null| 22.8|001000|\n| 10260|99999|20190103|31.7|29.1|1008.9| 994.7| 13.6|11.6| 21.4| 49.5|null|null|null|999.9|011000|\n| 10260|99999|20190104|32.9|30.3|1011.4| 997.1| 15.8| 4.9|  7.8| 10.9|36.1|31.8|null|999.9|001000|\n| 10260|99999|20190105|35.5|33.0|1015.7|1001.4| 12.0|10.4| 13.6| 21.0|null|32.7|null| 23.6|010000|\n| 10260|99999|20190106|38.5|34.1|1008.2| 994.2| 12.8|10.0| 17.5| 28.9|41.4|null|null| 23.2|010000|\n| 10260|99999|20190107|32.1|29.8| 996.8| 982.7|  6.9|11.3| 15.5| 28.6|null|30.4|null|999.9|001000|\n| 10260|99999|20190108|31.6|28.0| 997.4| 983.3| 22.9| 5.9| 11.7| 19.0|34.3|null|null|  0.4|011000|\n| 10260|99999|20190109|29.9|27.7|1011.6| 997.3| 29.8| 7.6| 15.2| 26.6|32.4|26.1|null| 23.6|001000|\n| 10260|99999|20190110|33.1|30.6| 979.1| 965.3|  5.3|17.8| 24.9| 41.8|41.4|null|null|999.9|011000|\n| 10260|99999|20190111|31.2|29.0| 975.0| 961.1|  5.6|11.6| 17.5| 38.9|null|null|null|  0.4|011100|\n| 10260|99999|20190112|28.3|26.1| 988.2| 974.1|  8.2| 8.1| 13.6| 38.5|null|null|null|999.9|001000|\n| 10260|99999|20190113|22.7|20.9| 977.1| 963.0| 26.6| 4.1|  7.8| 15.2|27.7|null|null|  0.4|001000|\n| 10260|99999|20190114|20.0|18.3| 984.3| 970.0| 43.1| 3.6|  9.7| 10.7|null|15.4|null| 38.6|000000|\n| 10260|99999|20190115|25.9|23.2| 991.3| 977.1| 16.0| 7.4| 13.8| 20.8|27.3|19.2|null|999.9|001000|\n| 10260|99999|20190116|24.8|21.8| 992.5| 978.2| 33.4| 2.7|  5.8|999.9|26.1|23.5|null| 35.4|001000|\n| 10260|99999|20190117|21.4|19.0| 989.8| 975.5| 10.4| 6.1|  8.9| 13.4|null|null|null| 35.0|001000|\n| 10260|99999|20190118|21.0|19.1| 994.4| 980.0| 13.8| 5.6|  7.8| 12.8|22.3|19.2|null| 35.0|001000|\n| 10260|99999|20190119|20.2|18.5|1000.8| 986.3| 33.9| 3.4|  7.8| 10.3|null|null|null| 35.8|001000|\n| 10260|99999|20190120|21.7|18.5|1009.0| 994.4| 32.1| 9.5| 14.6| 20.4|null|18.7|null|999.9|001000|\n+------+-----+--------+----+----+------+------+-----+----+-----+-----+----+----+----+-----+------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = nullify_missing_values(data)\n",
    "data = parse_date(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-----+-------------------+----+----+------+------+-----+----+-----+----+----+----+----+----+------+\n|STN---| WBAN|           YEARMODA|TEMP|DEWP|   SLP|   STP|VISIB|WDSP|MXSPD|GUST| MAX| MIN|PRCP|SNDP|FRSHTT|\n+------+-----+-------------------+----+----+------+------+-----+----+-----+----+----+----+----+----+------+\n| 10260|99999|2019-01-01 00:00:00|26.1|21.2|1001.9| 987.5| 20.6| 9.0| 15.9|29.7|29.8|null|null|18.5|001000|\n| 10260|99999|2019-01-02 00:00:00|24.9|22.1|1020.1|1005.5|  5.4| 5.6| 13.6|22.1|null|20.7|null|22.8|001000|\n| 10260|99999|2019-01-03 00:00:00|31.7|29.1|1008.9| 994.7| 13.6|11.6| 21.4|49.5|null|null|null|null|011000|\n| 10260|99999|2019-01-04 00:00:00|32.9|30.3|1011.4| 997.1| 15.8| 4.9|  7.8|10.9|36.1|31.8|null|null|001000|\n| 10260|99999|2019-01-05 00:00:00|35.5|33.0|1015.7|1001.4| 12.0|10.4| 13.6|21.0|null|32.7|null|23.6|010000|\n| 10260|99999|2019-01-06 00:00:00|38.5|34.1|1008.2| 994.2| 12.8|10.0| 17.5|28.9|41.4|null|null|23.2|010000|\n| 10260|99999|2019-01-07 00:00:00|32.1|29.8| 996.8| 982.7|  6.9|11.3| 15.5|28.6|null|30.4|null|null|001000|\n| 10260|99999|2019-01-08 00:00:00|31.6|28.0| 997.4| 983.3| 22.9| 5.9| 11.7|19.0|34.3|null|null| 0.4|011000|\n| 10260|99999|2019-01-09 00:00:00|29.9|27.7|1011.6| 997.3| 29.8| 7.6| 15.2|26.6|32.4|26.1|null|23.6|001000|\n| 10260|99999|2019-01-10 00:00:00|33.1|30.6| 979.1| 965.3|  5.3|17.8| 24.9|41.8|41.4|null|null|null|011000|\n| 10260|99999|2019-01-11 00:00:00|31.2|29.0| 975.0| 961.1|  5.6|11.6| 17.5|38.9|null|null|null| 0.4|011100|\n| 10260|99999|2019-01-12 00:00:00|28.3|26.1| 988.2| 974.1|  8.2| 8.1| 13.6|38.5|null|null|null|null|001000|\n| 10260|99999|2019-01-13 00:00:00|22.7|20.9| 977.1| 963.0| 26.6| 4.1|  7.8|15.2|27.7|null|null| 0.4|001000|\n| 10260|99999|2019-01-14 00:00:00|20.0|18.3| 984.3| 970.0| 43.1| 3.6|  9.7|10.7|null|15.4|null|38.6|000000|\n| 10260|99999|2019-01-15 00:00:00|25.9|23.2| 991.3| 977.1| 16.0| 7.4| 13.8|20.8|27.3|19.2|null|null|001000|\n| 10260|99999|2019-01-16 00:00:00|24.8|21.8| 992.5| 978.2| 33.4| 2.7|  5.8|null|26.1|23.5|null|35.4|001000|\n| 10260|99999|2019-01-17 00:00:00|21.4|19.0| 989.8| 975.5| 10.4| 6.1|  8.9|13.4|null|null|null|35.0|001000|\n| 10260|99999|2019-01-18 00:00:00|21.0|19.1| 994.4| 980.0| 13.8| 5.6|  7.8|12.8|22.3|19.2|null|35.0|001000|\n| 10260|99999|2019-01-19 00:00:00|20.2|18.5|1000.8| 986.3| 33.9| 3.4|  7.8|10.3|null|null|null|35.8|001000|\n| 10260|99999|2019-01-20 00:00:00|21.7|18.5|1009.0| 994.4| 32.1| 9.5| 14.6|20.4|null|18.7|null|null|001000|\n+------+-----+-------------------+----+----+------+------+-----+----+-----+----+----+----+----+----+------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_attr = countrylist.join(stationlist, on='COUNTRY_ABBR', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = data.join(\n",
    "    station_attr.withColumnRenamed('STN_NO', 'STN---'),\n",
    "    on='STN---',\n",
    "    how='left'\n",
    ").cache()"
   ]
  },
  {
   "source": [
    "Step 2 - Questions\n",
    "\n",
    "Using the global weather data, answer the following:\n",
    "\n",
    "1. Which country had the hottest average mean temperature over the year?\n",
    "2. Which country had the most consecutive days of tornadoes/funnel cloud\n",
    "formations?\n",
    "3. Which country had the second highest average mean wind speed over the year?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+-----------------+\n|        COUNTRY_FULL|     monthly_TEMP|\n+--------------------+-----------------+\n|            DJIBOUTI|90.23198015990424|\n|                CHAD|87.14639472082757|\n|               SUDAN|85.07542338470381|\n|               NIGER|85.03855115744923|\n| JUAN DE NOVA ISLAND|84.51182777650894|\n|                MALI| 84.4698450533835|\n|         EL SALVADOR|84.44803416216793|\n|              TUVALU|84.34886658865116|\n|BRITISH INDIAN OC...| 83.9194399820983|\n|             TOKELAU|83.84275418635283|\n|      CAYMAN ISLANDS|83.76236167953455|\n|        BURKINA FASO|83.75583106326813|\n|            MALDIVES|83.67978358497757|\n|           SINGAPORE|83.63143542755746|\n|ST. VINCENT AND T...|83.54223237327423|\n|            CAMBODIA|83.50356529178372|\n|    MARSHALL ISLANDS| 83.2364911782702|\n|          MICRONESIA|83.23250023814408|\n|             SENEGAL|83.13447681132101|\n|            KIRIBATI|83.07528320407322|\n+--------------------+-----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "data_full.withColumn(\n",
    "    'month', F.month(F.col('YEARMODA'))\n",
    ").groupBy(\n",
    "    'COUNTRY_FULL', 'month'\n",
    ").agg(\n",
    "    # possibly multiple weather stations (unverified)\n",
    "    # thus aggregate by monthly first\n",
    "    # ideally aggregate by day -> by month -> by year\n",
    "    # leaving that for later if there's time\n",
    "    F.mean('TEMP').alias('mean_monthly_TEMP')\n",
    ").groupBy(\n",
    "    'COUNTRY_FULL'\n",
    ").agg(\n",
    "    F.mean('mean_monthly_TEMP').alias('monthly_TEMP')\n",
    ").orderBy(\n",
    "    F.col('monthly_TEMP'), ascending=False\n",
    ").show()"
   ]
  },
  {
   "source": [
    "Q: Which country had the hottest average mean temperature over the year?\n",
    "\n",
    "A: DJIBOUTI\n",
    "\n",
    "makes, sense, it's in a dry part of Africa"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Which country had the most consecutive days of tornadoes/funnel cloud formations?\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+--------------------+---------------------+\n|        COUNTRY_FULL|      set_inter_date|is_single_consecutive|\n+--------------------+--------------------+---------------------+\n|             ARMENIA|                 [1]|                 true|\n|        SOUTH AFRICA|                 [1]|                 true|\n|               BURMA|                 [1]|                 true|\n|            CAMBODIA|                 [1]|                 true|\n|          BANGLADESH|                 [1]|                 true|\n|               JAPAN|                 [1]|                 true|\n|              UGANDA|        [1, 2, 3, 4]|                false|\n|SOUTH GEORGIA AND...|                 [1]|                 true|\n|          CAPE VERDE|              [1, 2]|                false|\n|NORTHERN MARIANA ...|                 [1]|                 true|\n|FALKLAND ISLANDS ...|                 [1]|                 true|\n|          MAURITANIA|                 [1]|                 true|\n|              JERSEY|                 [1]|                 true|\n|            MALDIVES|                 [1]|                 true|\n|SAO TOME AND PRIN...| [30, 1, 5, 2, 3, 4]|                false|\n|            TANZANIA|                 [1]|                 true|\n|              JORDAN|                 [1]|                 true|\n|             MAYOTTE|                 [1]|                 true|\n|        MAN  ISLE OF|                 [1]|                 true|\n|             LESOTHO|[9, 1, 2, 3, 7, 4...|                false|\n+--------------------+--------------------+---------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql import DataFrame\n",
    "from typing import List\n",
    "\n",
    "from etl.pyspark_utils import consecutive_date_diff_by\n",
    "\n",
    "# do we have daily observations for each country?\n",
    "# can only compute for consecutive dates\n",
    "country_date_diff = consecutive_date_diff_by(\n",
    "    data_full,\n",
    "    by_cols=['COUNTRY_FULL'],\n",
    "    date_col='YEARMODA'\n",
    ")\n",
    "\n",
    "# filter for countries with inter_date only null or 1\n",
    "country_single_consecutive = (\n",
    "    country_date_diff\n",
    "    .groupBy('COUNTRY_FULL')\n",
    "    .agg(F.collect_set('inter_date').alias('set_inter_date'))\n",
    ").withColumn(\n",
    "    'is_single_consecutive',\n",
    "    F.udf(lambda s: s == [1], BooleanType())(F.col('set_inter_date'))\n",
    ")\n",
    "country_single_consecutive.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_single_consecutive = data_full.join(\n",
    "    country_single_consecutive.filter(F.col('is_single_consecutive')),\n",
    "    on='COUNTRY_FULL',\n",
    "    how='inner'\n",
    ").withColumn(\n",
    "    'has_tornado_or_funnel',\n",
    "    F.udf(lambda s: s[5] == '1', BooleanType())('FRSHTT')\n",
    ").select('COUNTRY_FULL', 'YEARMODA', 'has_tornado_or_funnel').distinct()\n",
    "assert data_single_consecutive.count() > 0\n",
    "assert data_single_consecutive.filter(F.col('has_tornado_or_funnel')).count() > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------+-------------------+---------------------+----------+\n",
      "|COUNTRY_FULL|           YEARMODA|has_tornado_or_funnel|flag_group|\n",
      "+------------+-------------------+---------------------+----------+\n",
      "|     ARMENIA|2019-01-01 00:00:00|                false|         0|\n",
      "|     ARMENIA|2019-01-02 00:00:00|                false|         0|\n",
      "|     ARMENIA|2019-01-03 00:00:00|                false|         0|\n",
      "|     ARMENIA|2019-01-04 00:00:00|                false|         0|\n",
      "|     ARMENIA|2019-01-05 00:00:00|                false|         0|\n",
      "|     ARMENIA|2019-01-06 00:00:00|                false|         0|\n",
      "|     ARMENIA|2019-01-07 00:00:00|                false|         0|\n",
      "|     ARMENIA|2019-01-08 00:00:00|                false|         0|\n",
      "|     ARMENIA|2019-01-09 00:00:00|                false|         0|\n",
      "|     ARMENIA|2019-01-10 00:00:00|                false|         0|\n",
      "|     ARMENIA|2019-01-11 00:00:00|                false|         0|\n",
      "|     ARMENIA|2019-01-12 00:00:00|                false|         0|\n",
      "|     ARMENIA|2019-01-13 00:00:00|                false|         0|\n",
      "|     ARMENIA|2019-01-14 00:00:00|                false|         0|\n",
      "|     ARMENIA|2019-01-15 00:00:00|                false|         0|\n",
      "|     ARMENIA|2019-01-16 00:00:00|                false|         0|\n",
      "|     ARMENIA|2019-01-17 00:00:00|                false|         0|\n",
      "|     ARMENIA|2019-01-18 00:00:00|                false|         0|\n",
      "|     ARMENIA|2019-01-19 00:00:00|                false|         0|\n",
      "|     ARMENIA|2019-01-20 00:00:00|                false|         0|\n",
      "+------------+-------------------+---------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------------+-------------------+---------------------+----------+--------+--------+\n",
      "|COUNTRY_FULL|           YEARMODA|has_tornado_or_funnel|flag_group|streak_0|streak_1|\n",
      "+------------+-------------------+---------------------+----------+--------+--------+\n",
      "|     ARMENIA|2019-01-01 00:00:00|                false|         0|       1|       0|\n",
      "|     ARMENIA|2019-01-02 00:00:00|                false|         0|       2|       0|\n",
      "|     ARMENIA|2019-01-03 00:00:00|                false|         0|       3|       0|\n",
      "|     ARMENIA|2019-01-04 00:00:00|                false|         0|       4|       0|\n",
      "|     ARMENIA|2019-01-05 00:00:00|                false|         0|       5|       0|\n",
      "|     ARMENIA|2019-01-06 00:00:00|                false|         0|       6|       0|\n",
      "|     ARMENIA|2019-01-07 00:00:00|                false|         0|       7|       0|\n",
      "|     ARMENIA|2019-01-08 00:00:00|                false|         0|       8|       0|\n",
      "|     ARMENIA|2019-01-09 00:00:00|                false|         0|       9|       0|\n",
      "|     ARMENIA|2019-01-10 00:00:00|                false|         0|      10|       0|\n",
      "|     ARMENIA|2019-01-11 00:00:00|                false|         0|      11|       0|\n",
      "|     ARMENIA|2019-01-12 00:00:00|                false|         0|      12|       0|\n",
      "|     ARMENIA|2019-01-13 00:00:00|                false|         0|      13|       0|\n",
      "|     ARMENIA|2019-01-14 00:00:00|                false|         0|      14|       0|\n",
      "|     ARMENIA|2019-01-15 00:00:00|                false|         0|      15|       0|\n",
      "|     ARMENIA|2019-01-16 00:00:00|                false|         0|      16|       0|\n",
      "|     ARMENIA|2019-01-17 00:00:00|                false|         0|      17|       0|\n",
      "|     ARMENIA|2019-01-18 00:00:00|                false|         0|      18|       0|\n",
      "|     ARMENIA|2019-01-19 00:00:00|                false|         0|      19|       0|\n",
      "|     ARMENIA|2019-01-20 00:00:00|                false|         0|      20|       0|\n",
      "+------------+-------------------+---------------------+----------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/54445961/pyspark-calculate-streak-of-consecutive-observations\n",
    "# seems like i need a few windows ???\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "w_base = Window.partitionBy('COUNTRY_FULL').orderBy('YEARMODA')\n",
    "w_flagged = Window.partitionBy('COUNTRY_FULL', 'has_tornado_or_funnel').orderBy('YEARMODA')\n",
    "\n",
    "dftemp = data_single_consecutive.withColumn(\n",
    "    'flag_group',\n",
    "    # when 2 row counts increment, differences increase if there's a break\n",
    "    F.row_number().over(w_base) - F.row_number().over(w_flagged)\n",
    ")\n",
    "dftemp.show()\n",
    "\n",
    "# i don't really get the following part\n",
    "w_streak = Window.partitionBy('COUNTRY_FULL', 'has_tornado_or_funnel', 'flag_group').orderBy('YEARMODA')\n",
    "dftemp = (\n",
    "    dftemp\n",
    "    .withColumn(\n",
    "        'streak_0',\n",
    "        F.when(F.col('has_tornado_or_funnel'), 0).otherwise(F.row_number().over(w_streak))\n",
    "    )\n",
    "    .withColumn(\n",
    "        'streak_1',\n",
    "        F.when(~F.col('has_tornado_or_funnel'), 0).otherwise(F.row_number().over(w_streak))\n",
    "    )\n",
    ")\n",
    "dftemp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------+-------------------+---------------------+----------+--------+--------+\n|  COUNTRY_FULL|           YEARMODA|has_tornado_or_funnel|flag_group|streak_0|streak_1|\n+--------------+-------------------+---------------------+----------+--------+--------+\n|        CANADA|2019-06-26 00:00:00|                 true|       176|       0|       2|\n|         ITALY|2019-10-03 00:00:00|                 true|       275|       0|       2|\n|         JAPAN|2019-04-02 00:00:00|                 true|        91|       0|       1|\n|    BANGLADESH|2019-08-22 00:00:00|                 true|       233|       0|       1|\n|         MALTA|2019-01-19 00:00:00|                 true|        18|       0|       1|\n|         JAPAN|2019-12-04 00:00:00|                 true|       338|       0|       1|\n|         JAPAN|2019-12-03 00:00:00|                 true|       336|       0|       1|\n|        JERSEY|2019-06-12 00:00:00|                 true|       162|       0|       1|\n|UNITED KINGDOM|2019-06-12 00:00:00|                 true|       162|       0|       1|\n|         JAPAN|2019-01-24 00:00:00|                 true|        23|       0|       1|\n|         JAPAN|2019-06-11 00:00:00|                 true|       162|       0|       1|\n|         JAPAN|2019-03-02 00:00:00|                 true|        60|       0|       1|\n|         JAPAN|2019-08-30 00:00:00|                 true|       242|       0|       1|\n|      MALDIVES|2019-11-09 00:00:00|                 true|       312|       0|       1|\n|      TANZANIA|2019-05-17 00:00:00|                 true|       137|       0|       1|\n|UNITED KINGDOM|2019-09-07 00:00:00|                 true|       250|       0|       1|\n|UNITED KINGDOM|2019-06-20 00:00:00|                 true|       171|       0|       1|\n|         JAPAN|2019-01-17 00:00:00|                 true|        16|       0|       1|\n|         JAPAN|2019-12-14 00:00:00|                 true|       348|       0|       1|\n|         JAPAN|2019-06-10 00:00:00|                 true|       160|       0|       1|\n+--------------+-------------------+---------------------+----------+--------+--------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "dftemp.orderBy('streak_1', ascending=False).show()"
   ]
  },
  {
   "source": [
    "Q: Which country had the most consecutive days of tornadoes/funnel cloud formations?\n",
    "\n",
    "A: Seems to be CANADA. Not sure."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "----"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+------------------+\n|        COUNTRY_FULL|      monthly_WDSP|\n+--------------------+------------------+\n|FALKLAND ISLANDS ...|17.853323291643207|\n|               ARUBA|15.977568466445604|\n|       FAROE ISLANDS|15.004640085348584|\n|            BARBADOS|14.104861833330643|\n|ST. PIERRE AND MI...|13.964271671947186|\n|FRENCH SOUTHERN A...|13.721410918455398|\n|          CAPE VERDE|13.689624769963283|\n|          MAURITANIA|13.049711560945854|\n|     TROMELIN ISLAND|12.996336024862229|\n|          ST. HELENA| 12.70270694631401|\n|          ANTARCTICA|12.317377902614764|\n|             SOMALIA|12.270474730728104|\n|COCOS (KEELING) I...| 12.04136264134967|\n|            GUERNSEY|12.027518638882889|\n|        MAN  ISLE OF|11.902185717540952|\n|          MONTSERRAT|11.813225809104981|\n|             ICELAND|11.793342469451671|\n|      WESTERN SAHARA|11.652829126173785|\n|           ST. LUCIA|11.580525361820179|\n|            SVALBARD|11.489402906171904|\n+--------------------+------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "data_full.withColumn(\n",
    "    'month', F.month(F.col('YEARMODA'))\n",
    ").groupBy(\n",
    "    'COUNTRY_FULL', 'month'\n",
    ").agg(\n",
    "    F.mean('WDSP').alias('mean_monthly_WDSP')\n",
    ").groupBy(\n",
    "    'COUNTRY_FULL'\n",
    ").agg(\n",
    "    F.mean('mean_monthly_WDSP').alias('monthly_WDSP')\n",
    ").orderBy(\n",
    "    F.col('monthly_WDSP'), ascending=False\n",
    ").show()"
   ]
  },
  {
   "source": [
    "Q: Which country had the second highest average mean wind speed over the year?\n",
    "\n",
    "A: ARUBA|15.977568466445604\n",
    "\n",
    "Makes sense! It's right in the middle of the Caribbean, which has hurricanes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}